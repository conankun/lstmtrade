{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import indicators as idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>30.642857</td>\n",
       "      <td>30.340000</td>\n",
       "      <td>30.490000</td>\n",
       "      <td>123432400.0</td>\n",
       "      <td>30.572857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>30.798571</td>\n",
       "      <td>30.464285</td>\n",
       "      <td>30.657143</td>\n",
       "      <td>150476200.0</td>\n",
       "      <td>30.625713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>30.747143</td>\n",
       "      <td>30.107143</td>\n",
       "      <td>30.625713</td>\n",
       "      <td>138040000.0</td>\n",
       "      <td>30.138571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>30.285715</td>\n",
       "      <td>29.864286</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>119282800.0</td>\n",
       "      <td>30.082857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>30.285715</td>\n",
       "      <td>29.865715</td>\n",
       "      <td>30.042856</td>\n",
       "      <td>111902700.0</td>\n",
       "      <td>30.282858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 High        Low       Open       Volume  Adj Close\n",
       "Date                                                               \n",
       "2010-01-04  30.642857  30.340000  30.490000  123432400.0  30.572857\n",
       "2010-01-05  30.798571  30.464285  30.657143  150476200.0  30.625713\n",
       "2010-01-06  30.747143  30.107143  30.625713  138040000.0  30.138571\n",
       "2010-01-07  30.285715  29.864286  30.250000  119282800.0  30.082857\n",
       "2010-01-08  30.285715  29.865715  30.042856  111902700.0  30.282858"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/AAPL.csv\", index_col = 0)\n",
    "df[\"Adj Close\"] = df.Close # Moving close to the last column\n",
    "df.drop(['Close'], 1, inplace=True) # Moving close to the last column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.012862</td>\n",
       "      <td>0.015604</td>\n",
       "      <td>0.014799</td>\n",
       "      <td>0.244034</td>\n",
       "      <td>0.015330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.013620</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>0.015621</td>\n",
       "      <td>0.302982</td>\n",
       "      <td>0.015589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.013370</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>0.015466</td>\n",
       "      <td>0.275875</td>\n",
       "      <td>0.013208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.011124</td>\n",
       "      <td>0.013256</td>\n",
       "      <td>0.013618</td>\n",
       "      <td>0.234989</td>\n",
       "      <td>0.012936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.011124</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>0.012599</td>\n",
       "      <td>0.218903</td>\n",
       "      <td>0.013913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                High       Low      Open    Volume  Adj Close\n",
       "Date                                                         \n",
       "2010-01-04  0.012862  0.015604  0.014799  0.244034   0.015330\n",
       "2010-01-05  0.013620  0.016218  0.015621  0.302982   0.015589\n",
       "2010-01-06  0.013370  0.014455  0.015466  0.275875   0.013208\n",
       "2010-01-07  0.011124  0.013256  0.013618  0.234989   0.012936\n",
       "2010-01-08  0.011124  0.013263  0.012599  0.218903   0.013913"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_data(df):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    df['High'] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))\n",
    "    df['Low'] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))\n",
    "    df['Open'] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))\n",
    "    df['Volume'] = min_max_scaler.fit_transform(df.Volume.values.reshape(-1,1))\n",
    "    df['Adj Close'] = min_max_scaler.fit_transform(df['Adj Close'].values.reshape(-1,1))\n",
    "    return df\n",
    "df = normalize_data(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set and testing set\n",
    "\n",
    "def load_data(stock, seq_len):\n",
    "    features = len(stock.columns) # five features\n",
    "    data = stock.values\n",
    "    sequence_length = seq_len+1 # +1 because index starts from 0\n",
    "    result = []\n",
    "    # maximum date = latest date - sequence length\n",
    "    # there is at most 22 trading days in one month\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        # index: index + 22 days\n",
    "        result.append(data[index:index+sequence_length])\n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90 percent split\n",
    "    train = result[:int(row), :] # 90 percent of  date, all features\n",
    "    \n",
    "    x_train = train[:, :-1] \n",
    "    y_train = train[:,-1][:,-1]\n",
    "    \n",
    "    x_test = result[int(row):,:-1] \n",
    "    y_test = result[int(row):,-1][:,-1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], features))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "    # adam = keras.optimizers.Adam(decay=0.2)\n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 22\n",
    "X_train, y_train, X_test, y_test = load_data(df, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Compilation Time :  0.04490327835083008\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1795 samples, validate on 200 samples\n",
      "Epoch 1/90\n",
      "1795/1795 [==============================] - 3s 2ms/step - loss: 0.0828 - acc: 5.5710e-04 - val_loss: 0.2322 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1795/1795 [==============================] - 2s 1ms/step - loss: 0.0251 - acc: 5.5710e-04 - val_loss: 0.0303 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1795/1795 [==============================] - 2s 989us/step - loss: 0.0079 - acc: 5.5710e-04 - val_loss: 0.1067 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1795/1795 [==============================] - 2s 974us/step - loss: 0.0082 - acc: 5.5710e-04 - val_loss: 0.0466 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1795/1795 [==============================] - 2s 969us/step - loss: 0.0048 - acc: 5.5710e-04 - val_loss: 0.0357 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1795/1795 [==============================] - 2s 946us/step - loss: 0.0025 - acc: 5.5710e-04 - val_loss: 0.0472 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1795/1795 [==============================] - 2s 961us/step - loss: 0.0025 - acc: 5.5710e-04 - val_loss: 0.0184 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1795/1795 [==============================] - 2s 944us/step - loss: 0.0021 - acc: 5.5710e-04 - val_loss: 0.0248 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1795/1795 [==============================] - 2s 930us/step - loss: 0.0016 - acc: 5.5710e-04 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1795/1795 [==============================] - 2s 954us/step - loss: 0.0012 - acc: 5.5710e-04 - val_loss: 0.0159 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1795/1795 [==============================] - 2s 934us/step - loss: 0.0010 - acc: 5.5710e-04 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1795/1795 [==============================] - 2s 948us/step - loss: 9.8228e-04 - acc: 5.5710e-04 - val_loss: 0.0118 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1795/1795 [==============================] - 2s 930us/step - loss: 9.0266e-04 - acc: 5.5710e-04 - val_loss: 0.0150 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1795/1795 [==============================] - 2s 925us/step - loss: 8.3445e-04 - acc: 5.5710e-04 - val_loss: 0.0075 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1795/1795 [==============================] - 2s 936us/step - loss: 7.3678e-04 - acc: 5.5710e-04 - val_loss: 0.0092 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1795/1795 [==============================] - 2s 918us/step - loss: 7.0829e-04 - acc: 5.5710e-04 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1795/1795 [==============================] - 2s 938us/step - loss: 6.6312e-04 - acc: 5.5710e-04 - val_loss: 0.0076 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1795/1795 [==============================] - 2s 928us/step - loss: 6.8476e-04 - acc: 5.5710e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1795/1795 [==============================] - 2s 922us/step - loss: 6.9171e-04 - acc: 5.5710e-04 - val_loss: 0.0065 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1795/1795 [==============================] - 2s 957us/step - loss: 6.3944e-04 - acc: 5.5710e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1795/1795 [==============================] - 2s 925us/step - loss: 6.2100e-04 - acc: 5.5710e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1795/1795 [==============================] - 2s 950us/step - loss: 6.1603e-04 - acc: 5.5710e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1795/1795 [==============================] - 2s 924us/step - loss: 6.5347e-04 - acc: 5.5710e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1795/1795 [==============================] - 2s 932us/step - loss: 6.2418e-04 - acc: 5.5710e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1795/1795 [==============================] - 2s 956us/step - loss: 6.3863e-04 - acc: 5.5710e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1795/1795 [==============================] - 2s 921us/step - loss: 5.4442e-04 - acc: 5.5710e-04 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1795/1795 [==============================] - 2s 950us/step - loss: 5.8761e-04 - acc: 5.5710e-04 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1795/1795 [==============================] - 2s 922us/step - loss: 5.8119e-04 - acc: 5.5710e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1795/1795 [==============================] - 2s 923us/step - loss: 5.9381e-04 - acc: 5.5710e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1795/1795 [==============================] - 2s 957us/step - loss: 5.6406e-04 - acc: 5.5710e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1795/1795 [==============================] - 2s 916us/step - loss: 5.6316e-04 - acc: 5.5710e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1795/1795 [==============================] - 2s 957us/step - loss: 5.4941e-04 - acc: 5.5710e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1795/1795 [==============================] - 2s 945us/step - loss: 5.4020e-04 - acc: 5.5710e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1795/1795 [==============================] - 2s 975us/step - loss: 5.6279e-04 - acc: 5.5710e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1795/1795 [==============================] - 2s 941us/step - loss: 5.6306e-04 - acc: 5.5710e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1795/1795 [==============================] - 2s 913us/step - loss: 5.2163e-04 - acc: 5.5710e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1795/1795 [==============================] - 2s 936us/step - loss: 5.3057e-04 - acc: 5.5710e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1795/1795 [==============================] - 2s 916us/step - loss: 5.4442e-04 - acc: 5.5710e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1795/1795 [==============================] - 2s 929us/step - loss: 5.2340e-04 - acc: 5.5710e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1795/1795 [==============================] - 2s 927us/step - loss: 5.0232e-04 - acc: 5.5710e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1795/1795 [==============================] - 2s 912us/step - loss: 5.3849e-04 - acc: 5.5710e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1795/1795 [==============================] - 2s 943us/step - loss: 4.7924e-04 - acc: 5.5710e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1795/1795 [==============================] - 2s 911us/step - loss: 4.8055e-04 - acc: 5.5710e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1795/1795 [==============================] - 2s 930us/step - loss: 5.1951e-04 - acc: 5.5710e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1795/1795 [==============================] - 2s 930us/step - loss: 4.5433e-04 - acc: 5.5710e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1795/1795 [==============================] - 2s 916us/step - loss: 4.5479e-04 - acc: 5.5710e-04 - val_loss: 8.9080e-04 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1795/1795 [==============================] - 2s 955us/step - loss: 4.7105e-04 - acc: 5.5710e-04 - val_loss: 7.1296e-04 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1795/1795 [==============================] - 2s 911us/step - loss: 4.7779e-04 - acc: 5.5710e-04 - val_loss: 5.6407e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1795/1795 [==============================] - 2s 933us/step - loss: 4.7353e-04 - acc: 5.5710e-04 - val_loss: 5.3961e-04 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1795/1795 [==============================] - 2s 928us/step - loss: 4.6433e-04 - acc: 5.5710e-04 - val_loss: 6.7177e-04 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1795/1795 [==============================] - 2s 911us/step - loss: 4.4413e-04 - acc: 5.5710e-04 - val_loss: 7.2419e-04 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1795/1795 [==============================] - 2s 948us/step - loss: 4.5780e-04 - acc: 5.5710e-04 - val_loss: 5.0566e-04 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1795/1795 [==============================] - 2s 915us/step - loss: 4.4367e-04 - acc: 5.5710e-04 - val_loss: 4.6341e-04 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1795/1795 [==============================] - 2s 937us/step - loss: 5.3737e-04 - acc: 5.5710e-04 - val_loss: 6.3535e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1795/1795 [==============================] - 2s 917us/step - loss: 5.0171e-04 - acc: 5.5710e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/90\n",
      "1795/1795 [==============================] - 2s 920us/step - loss: 5.6841e-04 - acc: 5.5710e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1795/1795 [==============================] - 2s 944us/step - loss: 4.7701e-04 - acc: 5.5710e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1795/1795 [==============================] - 2s 921us/step - loss: 5.1952e-04 - acc: 5.5710e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1795/1795 [==============================] - 2s 940us/step - loss: 5.1493e-04 - acc: 5.5710e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1795/1795 [==============================] - 2s 923us/step - loss: 4.6513e-04 - acc: 5.5710e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1795/1795 [==============================] - 2s 910us/step - loss: 4.5331e-04 - acc: 5.5710e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1795/1795 [==============================] - 2s 942us/step - loss: 4.9815e-04 - acc: 5.5710e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1795/1795 [==============================] - 2s 914us/step - loss: 5.4634e-04 - acc: 5.5710e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1795/1795 [==============================] - 2s 941us/step - loss: 4.9102e-04 - acc: 5.5710e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1795/1795 [==============================] - 2s 923us/step - loss: 4.3691e-04 - acc: 5.5710e-04 - val_loss: 5.3867e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1795/1795 [==============================] - 2s 913us/step - loss: 4.4217e-04 - acc: 5.5710e-04 - val_loss: 4.0342e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1795/1795 [==============================] - 2s 944us/step - loss: 4.1454e-04 - acc: 5.5710e-04 - val_loss: 4.0028e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1795/1795 [==============================] - 2s 913us/step - loss: 4.3018e-04 - acc: 5.5710e-04 - val_loss: 4.0415e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1795/1795 [==============================] - 2s 935us/step - loss: 3.9919e-04 - acc: 5.5710e-04 - val_loss: 5.9780e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1795/1795 [==============================] - 2s 916us/step - loss: 3.9530e-04 - acc: 5.5710e-04 - val_loss: 4.0182e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1795/1795 [==============================] - 2s 922us/step - loss: 4.2866e-04 - acc: 5.5710e-04 - val_loss: 3.8944e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1795/1795 [==============================] - 2s 946us/step - loss: 4.1255e-04 - acc: 5.5710e-04 - val_loss: 5.2358e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1795/1795 [==============================] - 2s 913us/step - loss: 4.5531e-04 - acc: 5.5710e-04 - val_loss: 4.4571e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1795/1795 [==============================] - 2s 956us/step - loss: 4.3324e-04 - acc: 5.5710e-04 - val_loss: 5.3843e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1795/1795 [==============================] - 2s 917us/step - loss: 3.8967e-04 - acc: 5.5710e-04 - val_loss: 4.8706e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1795/1795 [==============================] - 2s 910us/step - loss: 3.8144e-04 - acc: 5.5710e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1795/1795 [==============================] - 2s 942us/step - loss: 4.2230e-04 - acc: 5.5710e-04 - val_loss: 6.0517e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1795/1795 [==============================] - 2s 907us/step - loss: 4.0054e-04 - acc: 5.5710e-04 - val_loss: 3.5674e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1795/1795 [==============================] - 2s 1ms/step - loss: 4.0810e-04 - acc: 5.5710e-04 - val_loss: 3.5460e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1795/1795 [==============================] - 2s 962us/step - loss: 3.8298e-04 - acc: 5.5710e-04 - val_loss: 5.7079e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1795/1795 [==============================] - 2s 930us/step - loss: 4.3950e-04 - acc: 5.5710e-04 - val_loss: 7.0540e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1795/1795 [==============================] - 2s 913us/step - loss: 4.0467e-04 - acc: 5.5710e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1795/1795 [==============================] - 2s 913us/step - loss: 4.2902e-04 - acc: 5.5710e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1795/1795 [==============================] - 2s 943us/step - loss: 4.6003e-04 - acc: 5.5710e-04 - val_loss: 9.6450e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1795/1795 [==============================] - 2s 915us/step - loss: 3.9829e-04 - acc: 5.5710e-04 - val_loss: 8.5768e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1795/1795 [==============================] - 2s 926us/step - loss: 3.7325e-04 - acc: 5.5710e-04 - val_loss: 5.6685e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1795/1795 [==============================] - 2s 926us/step - loss: 3.8039e-04 - acc: 5.5710e-04 - val_loss: 3.3657e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1795/1795 [==============================] - 2s 920us/step - loss: 3.8500e-04 - acc: 5.5710e-04 - val_loss: 4.3874e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1795/1795 [==============================] - 2s 942us/step - loss: 3.8031e-04 - acc: 5.5710e-04 - val_loss: 3.9947e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1795/1795 [==============================] - 2s 908us/step - loss: 3.8603e-04 - acc: 5.5710e-04 - val_loss: 7.9918e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29fa1904be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=90,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "# for each data index in test data\n",
    "for i in range(len(y_test)):\n",
    "    # pr = prediction day i\n",
    "    pr = p[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AAPL.csv\", index_col = 0)\n",
    "df[\"Adj Close\"] = df.Close # Moving close to the last column\n",
    "df.drop(['Close'], 1, inplace=True) # Moving close to the last column\n",
    "\n",
    "# Bug fixed at here, please update the denormalize function to this one\n",
    "def denormalize(df, normalized_value): \n",
    "    df = df['Adj Close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new\n",
    "\n",
    "newp = denormalize(df, p)\n",
    "newy_test = denormalize(df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00029 MSE (0.02 RMSE)\n",
      "Test Score: 0.00243 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00028986332612462287, 0.0024272353191197187)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11624499.816894531"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def investment(y_pred, y_test, cash=1000000, amount=1000):\n",
    "    # SHORT: amount / LONG: amount\n",
    "    # Invest based on y_pred\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = pd.Series(y_pred)\n",
    "    y_test = y_test.flatten()\n",
    "    y_test = pd.Series(y_test)\n",
    "    momentums = idx.momentum(y_pred, 50)\n",
    "    shares = 0\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        if momentums[i] > 0:\n",
    "            # BUY\n",
    "            if shares == 0 or shares == -amount:\n",
    "                cash -= y_test[i] * (amount - shares)\n",
    "                shares = amount\n",
    "        elif momentums[i] < 0:\n",
    "            # SELL\n",
    "            if shares == 0 or shares == amount:\n",
    "                cash += y_test[i] * (amount + shares)\n",
    "                shares = -amount\n",
    "    return cash + shares * y_test[y_pred.shape[0]-1]\n",
    "            \n",
    "investment(newp, newy_test, cash=10000000, amount=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot(newp,color='orange', label='Prediction')\n",
    "plt2.plot(newy_test,color='green', label='Actual')\n",
    "plt2.legend(loc='best')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
